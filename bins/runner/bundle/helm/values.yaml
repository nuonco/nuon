---
# set elsewhere
# runner_group_id:
env:
  NUON_DEBUG: 'true'
  RUNNER_ID: <must-be-set>
  API_TOKEN: <must-be-set>
  API_URL: <must-be-set>

image:
  repository: ""
  tag: "latest"

serviceAccount:
  name: runner
  annotations: {}

autoscaling:
  minReplicas: 1
  maxReplicas: 1
  targetCPUUtilizationPercentage: 100
  targetMemoryUtilizationPercentage: 100

# limits should be based on the instance_types
# docs: https://karpenter.sh/v0.37/reference/instance-types/#t3amedium
resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limit:
    cpu: 1750m
    memory: 3072Mi

node_pool:
  # if we want the deployment to create and run on its own NodePool, we must enable this
  # if this is false, we fall back to the default NodePool
  enabled: false
  capacity_types:
    - on-demand
  # docs: https://karpenter.sh/v0.37/reference/instance-types/#t3amedium
  instance_type:
    name: t3a.medium
    cpu: 2
    memory: 4096  # int so we can do math, rendered with Mi

  # the max number of nodes is controlled by this
  # these should be set like this:
  #   cpu: (instance_type.cpu * desired_runners)  + "some percentage to smooth bursty scaling"
  #   memory: instance_type.cpu * desired_runners +
  #   this is too loose a coupling IMO
  runner_count: 1
